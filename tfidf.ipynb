{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a64a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48bfd980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1562562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERMITTED_TITLES_SOURCE = \"scientific-paper-summarisation/Data/Utility_Data/permitted_titles.txt\"\n",
    "non_content_keys = ['MAIN-TITLE', 'HIGHLIGHTS', 'KEYPHRASES', 'ABSTRACT', 'ACKNOWLEDGEMENTS', 'REFERENCES']\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7828d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, filter_sentence=True):\n",
    "    \"\"\"\n",
    "    Preprocesses a sentence, turning it all to lowercase and tokenizing it into words.\n",
    "    :param sentence: the sentence to pre-process.\n",
    "    :return: the sentence, as a list of words, all in lowercase\n",
    "    \"\"\"\n",
    "    \n",
    "    if filter_sentence:\n",
    "        sentence = sentence.lower()\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "\n",
    "        # Remove stopwords from sentence\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words and w.isalnum()]\n",
    "        return filtered_sentence\n",
    "    \n",
    "    # Remove all line endings, multiple whitespace etc. from sentence\n",
    "    cleaned_sentence = ' '.join(sentence.split())    \n",
    "    return cleaned_sentence\n",
    "\n",
    "def paper_tokenize(text, sentences_as_lists=False, preserve_order=False):\n",
    "    \"\"\"\n",
    "    Takes a paper with the sections delineated by '@&#' and splits them into a dictionary where the key is the section\n",
    "    and the value is the text under that section. This could probably be a bit more efficient but it works well enough.\n",
    "    :param text: the text of the paper to split\n",
    "    :param sentences_as_lists: if true, returns the text of each section as a list of sentences rather than a single\n",
    "                               string.\n",
    "    :param preserve_order: if true, tracks the order in which the paper sections occured.\n",
    "    :returns: a dictionary of the form (section: section_text)\n",
    "    \"\"\"\n",
    "    with open(PERMITTED_TITLES_SOURCE, \"r\") as pt:\n",
    "        permitted_titles = pt.read().split(\"\\n\")\n",
    "\n",
    "    # Split the text into sections\n",
    "    if preserve_order:\n",
    "        split_text_1 = re.split(\"@&#\", text)\n",
    "        split_text = zip(split_text_1, range(len(split_text_1)))\n",
    "    else:\n",
    "        split_text = re.split(\"@&#\", text)\n",
    "\n",
    "    # The key value. This value is changed if a permitted section title is encountered in the list.\n",
    "    state = \"\"\n",
    "\n",
    "    # After the for loop, this dictionary will have keys relating to each permitted section, and values corresponding\n",
    "    # to the text of that section\n",
    "    sentences_with_states = defaultdict(str)\n",
    "    sentences = defaultdict(str)\n",
    "\n",
    "    section_counts = defaultdict(int)\n",
    "    \n",
    "    paper_abstract = \"\"\n",
    "    sentence_index = 0\n",
    "    \n",
    "    if preserve_order:\n",
    "        for text, pos in split_text:\n",
    "\n",
    "            # Hack for proper sentence tokenization because NLTK tokeniser doesn't work properly for tokenising papers\n",
    "            text = text.replace(\"etal.\", \"etal\")\n",
    "            text = text.replace(\"et al.\", \"etal\")\n",
    "            text = text.replace(\"Fig.\", \"Fig\")\n",
    "            text = text.replace(\"fig.\", \"fig\")\n",
    "            text = text.replace(\"Eq.\", \"Eq\")\n",
    "            text = text.replace(\"eq.\", \"eq\")\n",
    "            text = text.replace(\"pp.\", \"pp\")\n",
    "            text = text.replace(\"i.e.\", \"ie\")\n",
    "            text = text.replace(\"e.g.\", \"eg\")\n",
    "            text = text.replace(\"ref.\", \"ref\")\n",
    "            text = text.replace(\"Ref.\", \"Ref\")\n",
    "            text = text.replace(\"etc.\", \"etc\")\n",
    "            text = text.replace(\"Figs.\", \"Figs\")\n",
    "            text = text.replace(\"figs.\", \"figs\")\n",
    "            text = text.replace(\"No.\", \"No\")\n",
    "            text = text.replace(\"eqs.\", \"eqs\")\n",
    "\n",
    "            # Checks if text is a section title\n",
    "            if text.lower() in permitted_titles:\n",
    "                state = text\n",
    "                section_counts[state] += 1\n",
    "            else:\n",
    "                if sentences_as_lists:\n",
    "                    if section_counts[state] > 1:\n",
    "                        state = state + \"_\" + str(section_counts[state])\n",
    "                    sentences_with_states[state] = ([preprocess_sentence(x) for x in sent_tokenize(text)], pos)\n",
    "                    \n",
    "                    sentence_storage = []\n",
    "                    for x in sent_tokenize(text):\n",
    "                        sentence_storage.append((preprocess_sentence(x, filter_sentence=False), sentence_index))\n",
    "                        sentence_index+=1\n",
    "                    sentences[state] = sentence_storage\n",
    "            if state == \"ABSTRACT\":\n",
    "                paper_abstract = text.strip()\n",
    "\n",
    "    return sentences, sentences_with_states, paper_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cd58e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_as_words(tokenized_paper):\n",
    "    all_words = []\n",
    "    for key in tokenized_paper.keys():\n",
    "        \n",
    "        # For every paper section that contains content information,\n",
    "        # retrieve words\n",
    "        if key not in non_content_keys:\n",
    "            section_content = tokenized_paper[key]\n",
    "            [all_words.extend(s) for s in section_content[0]]\n",
    "            \n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d950455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_freq(words, sentences):\n",
    "    freq_dict = {}\n",
    "    for word in words:\n",
    "        freq_dict[word] = sum(1 for sent in sentences if word in sent)\n",
    "    return freq_dict\n",
    "\n",
    "def compute_tf(word: str, sentence: List):\n",
    "    freq = sum(1 for sent_word in sentence if sent_word == word)\n",
    "    return freq / len(sentence)\n",
    "\n",
    "def compute_idf(word: str, no_sentences: int, freq_dict: Counter):\n",
    "    sentence_freq = freq_dict[word]\n",
    "    \n",
    "    return math.log10(no_sentences / sentence_freq)\n",
    "\n",
    "def compute_tfidf(tf: float, idf: float):\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0fe3857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_weight(sentence, dict_freq, no_sentences):    \n",
    "    sentence_score = 0\n",
    "    for word in sentence:\n",
    "        tf = compute_tf(word, sentence)\n",
    "        idf = compute_idf(word, no_sentences, dict_freq)\n",
    "        sentence_score += compute_tfidf(tf, idf)\n",
    "        \n",
    "    return sentence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86920414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_paper(tokenized_paper, paper_sentences, nr_sentences=5):\n",
    "    sentence_weights = []\n",
    "    \n",
    "    # Get word representation of paper\n",
    "    paper_words = get_paper_as_words(tokenized_paper)\n",
    "    \n",
    "    processed_sentences = []\n",
    "    original_sentences = []\n",
    "    \n",
    "    # Get all sentences in paper\n",
    "    for section in tokenized_paper.keys():\n",
    "        \n",
    "        if section not in non_content_keys:\n",
    "            processed_sentences.extend(tokenized_paper[section][0])\n",
    "            original_sentences.extend(paper_sentences[section])\n",
    "    \n",
    "    # For every word, compute how often they appear in a sentence\n",
    "    freq_dict = compute_sentence_freq(paper_words, processed_sentences)\n",
    "    no_sentences = len(processed_sentences)\n",
    "    \n",
    "    for tok_sentence, orig_sentence in zip(processed_sentences, original_sentences):\n",
    "        \n",
    "        # Compute sentence weight and store with sentence\n",
    "        sentence_weight = compute_sentence_weight(tok_sentence, freq_dict, no_sentences)\n",
    "        sentence_weights.append((orig_sentence[0], orig_sentence[1], sentence_weight))\n",
    "            \n",
    "    # Create a dataframe of all sentences and sort descending by weight\n",
    "    sentence_weights = pd.DataFrame(sentence_weights, columns=['sentence', 'index', 'weight'])\n",
    "    sentence_weights.sort_values(by=['weight'], ascending=False, inplace=True)\n",
    "    \n",
    "    # Select desired number of sentences and sort by order of occurence in text\n",
    "    summary = sentence_weights.head(nr_sentences).sort_values(by=['index'])['sentence'].values\n",
    "    \n",
    "    # Join selected strings into a summary\n",
    "    string_summary = ' '.join(summary)\n",
    "    \n",
    "    return string_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3f8171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "def compute_metrics(paper_abstract: np.array, generated_summary: np.array):\n",
    "    rouge_scores = rouge.get_scores(generated_summary, paper_abstract, avg=True)\n",
    "    print(rouge_scores)\n",
    "#     return rouge_scores['rouge-1'].values(), rouge_scores['rouge-2'].values(), rouge_scores['rouge-l'].values(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26b0290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [00:09,  2.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8600/3727632096.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Summarize paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mgenerated_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_paper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_paper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaper_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNR_OF_SENTENCES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mgenerated_summaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8600/3449364755.py\u001b[0m in \u001b[0;36msummarize_paper\u001b[0;34m(tokenized_paper, paper_sentences, nr_sentences)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# For every word, compute how often they appear in a sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mfreq_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_sentence_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mno_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8600/981170601.py\u001b[0m in \u001b[0;36mcompute_sentence_freq\u001b[0;34m(words, sentences)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfreq_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mfreq_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfreq_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8600/981170601.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfreq_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mfreq_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfreq_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PAPER_PATH = 'scientific-paper-summarisation/Data/Parsed_Papers'\n",
    "paper_file_names = os.listdir('scientific-paper-summarisation/Data/Parsed_Papers')\n",
    "\n",
    "# Define desired number of sentences for a summary\n",
    "NR_OF_SENTENCES = 5\n",
    "\n",
    "ground_truth_summaries = np.empty(len(paper_file_names), dtype='object')\n",
    "generated_summaries = np.empty(len(paper_file_names), dtype='object')\n",
    "\n",
    "for i, paper_file_name in tqdm(enumerate(paper_file_names)):    \n",
    "    # Read paper file\n",
    "    filepath = f'{PAPER_PATH}/{paper_file_name}'\n",
    "    with open(filepath, \"r\") as paper_file:\n",
    "        paper_content = paper_file.read()\n",
    "    \n",
    "    # Tokenize paper into sentences (and sentences into separate words) and get paper abstract\n",
    "    paper_sentences, tokenized_paper, paper_abstract = paper_tokenize(paper_content, sentences_as_lists=True, preserve_order=True)\n",
    "    ground_truth_summaries[i] = paper_abstract\n",
    "    \n",
    "    # Summarize paper\n",
    "    generated_summary = summarize_paper(tokenized_paper, paper_sentences, NR_OF_SENTENCES)\n",
    "    generated_summaries[i] = generated_summary\n",
    "    \n",
    "# Compute ROUGE scores\n",
    "compute_metrics(ground_truth_summaries, generated_summaries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
