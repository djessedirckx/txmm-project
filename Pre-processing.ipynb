{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a64a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acff58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1562562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERMITTED_TITLES_SOURCE = \"scientific-paper-summarisation/Data/Utility_Data/permitted_titles.txt\"\n",
    "non_content_keys = ['MAIN-TITLE', 'HIGHLIGHTS', 'KEYPHRASES', 'ABSTRACT', 'ACKNOWLEDGEMENTS', 'REFERENCES']\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7828d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, filter_sentence=True):\n",
    "    \"\"\"\n",
    "    Preprocesses a sentence, turning it all to lowercase and tokenizing it into words.\n",
    "    :param sentence: the sentence to pre-process.\n",
    "    :return: the sentence, as a list of words, all in lowercase\n",
    "    \"\"\"\n",
    "    \n",
    "    if filter_sentence:\n",
    "        sentence = sentence.lower()\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "\n",
    "        # Remove stopwords from sentence\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words and w.isalnum()]\n",
    "        return filtered_sentence\n",
    "    \n",
    "    # Remove all line endings, multiple whitespace etc. from sentence\n",
    "    cleaned_sentence = ' '.join(sentence.split())    \n",
    "    return cleaned_sentence\n",
    "\n",
    "def paper_tokenize(text, sentences_as_lists=False, preserve_order=False):\n",
    "    \"\"\"\n",
    "    Takes a paper with the sections delineated by '@&#' and splits them into a dictionary where the key is the section\n",
    "    and the value is the text under that section. This could probably be a bit more efficient but it works well enough.\n",
    "    :param text: the text of the paper to split\n",
    "    :param sentences_as_lists: if true, returns the text of each section as a list of sentences rather than a single\n",
    "                               string.\n",
    "    :param preserve_order: if true, tracks the order in which the paper sections occured.\n",
    "    :returns: a dictionary of the form (section: section_text)\n",
    "    \"\"\"\n",
    "    with open(PERMITTED_TITLES_SOURCE, \"r\") as pt:\n",
    "        permitted_titles = pt.read().split(\"\\n\")\n",
    "\n",
    "    # Split the text into sections\n",
    "    if preserve_order:\n",
    "        split_text_1 = re.split(\"@&#\", text)\n",
    "        split_text = zip(split_text_1, range(len(split_text_1)))\n",
    "    else:\n",
    "        split_text = re.split(\"@&#\", text)\n",
    "\n",
    "    # The key value. This value is changed if a permitted section title is encountered in the list.\n",
    "    state = \"\"\n",
    "\n",
    "    # After the for loop, this dictionary will have keys relating to each permitted section, and values corresponding\n",
    "    # to the text of that section\n",
    "    sentences_with_states = defaultdict(str)\n",
    "    sentences = defaultdict(str)\n",
    "\n",
    "    section_counts = defaultdict(int)\n",
    "    \n",
    "    paper_abstract = \"\"\n",
    "    sentence_index = 0\n",
    "    \n",
    "    if preserve_order:\n",
    "        for text, pos in split_text:\n",
    "\n",
    "            # Hack for proper sentence tokenization because NLTK tokeniser doesn't work properly for tokenising papers\n",
    "            text = text.replace(\"etal.\", \"etal\")\n",
    "            text = text.replace(\"et al.\", \"etal\")\n",
    "            text = text.replace(\"Fig.\", \"Fig\")\n",
    "            text = text.replace(\"fig.\", \"fig\")\n",
    "            text = text.replace(\"Eq.\", \"Eq\")\n",
    "            text = text.replace(\"eq.\", \"eq\")\n",
    "            text = text.replace(\"pp.\", \"pp\")\n",
    "            text = text.replace(\"i.e.\", \"ie\")\n",
    "            text = text.replace(\"e.g.\", \"eg\")\n",
    "            text = text.replace(\"ref.\", \"ref\")\n",
    "            text = text.replace(\"Ref.\", \"Ref\")\n",
    "            text = text.replace(\"etc.\", \"etc\")\n",
    "            text = text.replace(\"Figs.\", \"Figs\")\n",
    "            text = text.replace(\"figs.\", \"figs\")\n",
    "            text = text.replace(\"No.\", \"No\")\n",
    "            text = text.replace(\"eqs.\", \"eqs\")\n",
    "\n",
    "            # Checks if text is a section title\n",
    "            if text.lower() in permitted_titles:\n",
    "                state = text\n",
    "                section_counts[state] += 1\n",
    "            else:\n",
    "                if sentences_as_lists:\n",
    "                    if section_counts[state] > 1:\n",
    "                        state = state + \"_\" + str(section_counts[state])\n",
    "                    sentences_with_states[state] = ([preprocess_sentence(x) for x in sent_tokenize(text)], pos)\n",
    "                    \n",
    "                    sentence_storage = []\n",
    "                    for x in sent_tokenize(text):\n",
    "                        sentence_storage.append((preprocess_sentence(x, filter_sentence=False), sentence_index))\n",
    "                        sentence_index+=1\n",
    "                    sentences[state] = sentence_storage\n",
    "            if state == \"ABSTRACT\":\n",
    "                paper_abstract = text.strip()\n",
    "\n",
    "    return sentences, sentences_with_states, paper_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cd58e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_as_words(tokenized_paper):\n",
    "    all_words = []\n",
    "    for key in tokenized_paper.keys():\n",
    "        \n",
    "        # For every paper section that contains content information,\n",
    "        # retrieve words\n",
    "        if key not in non_content_keys:\n",
    "            section_content = tokenized_paper[key]\n",
    "            [all_words.extend(s) for s in section_content[0]]\n",
    "            \n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c175dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_weight(sentence: List, text: List) -> int:\n",
    "    \n",
    "    sentence_score = 0\n",
    "    \n",
    "    # Iterate over all words in the sentence\n",
    "    for word in sentence:\n",
    "        \n",
    "        # Get word synsets\n",
    "        word_synsets = wn.synsets(word)\n",
    "        \n",
    "        # If word has synsets (i.e., is known by wordnet), continue\n",
    "        best_synset_score = 0\n",
    "        for synset in word_synsets:\n",
    "            \n",
    "            # Get and tokenize gloss and remove stopwords and punctuation\n",
    "            filtered_gloss = preprocess_sentence(synset.definition())\n",
    "            \n",
    "            # Compute score\n",
    "            score = sum(1 for def_word in filtered_gloss if def_word in text)\n",
    "            if score > best_synset_score:\n",
    "                best_synset_score = score\n",
    "                \n",
    "        # Update sentence score\n",
    "        sentence_score += best_synset_score\n",
    "\n",
    "    return sentence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86920414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_paper(tokenized_paper, paper_sentences, nr_sentences=5):\n",
    "    sentence_weights = []\n",
    "    \n",
    "    # Get word representation of paper\n",
    "    paper_words = get_paper_as_words(tokenized_paper)\n",
    "    \n",
    "    for section in tokenized_paper.keys():\n",
    "        \n",
    "        if section not in non_content_keys:\n",
    "            section_content = tokenized_paper[section]\n",
    "            section_sentences = paper_sentences[section]\n",
    "            \n",
    "            for tok_sentence, orig_sentence in zip(section_content[0], section_sentences):\n",
    "                \n",
    "                # Compute sentence weight and store with sentence\n",
    "                sentence_weight = compute_sentence_weight(tok_sentence, paper_words)\n",
    "                sentence_weights.append((orig_sentence[0], orig_sentence[1], sentence_weight))\n",
    "            \n",
    "    # Create a dataframe of all sentences and sort descending by weight\n",
    "    sentence_weights = pd.DataFrame(sentence_weights, columns=['sentence', 'index', 'weight'])\n",
    "    sentence_weights.sort_values(by=['weight'], ascending=False, inplace=True)\n",
    "    \n",
    "    # Select desired number of sentences and sort by order of occurence in text\n",
    "    summary = sentence_weights.head(nr_sentences).sort_values(by=['index'])['sentence'].values\n",
    "    \n",
    "    # Join selected strings into a summary\n",
    "    string_summary = ' '.join(summary)\n",
    "    \n",
    "    return string_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54d24046",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "def compute_metrics(paper_abstract: np.array, generated_summary: np.array):\n",
    "    rouge_scores = rouge.get_scores(generated_summary, paper_abstract, avg=True)\n",
    "    print(rouge_scores)\n",
    "#     return rouge_scores['rouge-1'].values(), rouge_scores['rouge-2'].values(), rouge_scores['rouge-l'].values(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26b0290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0142694X1500054X.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.14832535885167464, 'p': 0.22302158273381295, 'f': 0.17816091474253548}, 'rouge-2': {'r': 0.05187319884726225, 'p': 0.09045226130653267, 'f': 0.06593406130144004}, 'rouge-l': {'r': 0.1339712918660287, 'p': 0.2014388489208633, 'f': 0.16091953543219065}}\n",
      "['The average age of the graphic designers surveyed was 39, ranging from 18 to 71 years old, and the average amount of graphic design experience was 13 years, ranging from 1 year or less, to 43 years. The graphic designers were asked the extent to which they agreed with the statement ‘visual accessibility is very important in my day to day graphic design work’, while clients were asked about their agreement with the statement ‘visual accessibility is very important in all graphic design work that I commission’. Inclusive design is ‘a general approach to designing in which designers ensure that their products and services address the needs of the widest possible audience, irrespective of age or ability’ (Design Council, 2008). There is a statistically significant difference between the proportion of projects on which graphic designers recommend that visual accessibility be considered (mean 51.5%), and the proportion of projects on which clients report that graphic designers recommend that visual accessibility be considered (mean 33.3%), (χ2 test, p < 0.01), (See Figure 4 a). There are a variety of tools and methods to assist designers from all disciplines when considering their users, such as the Inclusive Design Toolkit (Clarkson, Coleman, Hosking, & Waller, 2011), Userfit (Poulson, Ashby, & Richardson, 1996), The Universal Design Handbook (Preiser & Ostroff, 2001) and the methods proposed by Stanford D School (Stanford University, 2014).']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PAPER_PATH = 'scientific-paper-summarisation/Data/Parsed_Papers'\n",
    "paper_file_names = os.listdir('scientific-paper-summarisation/Data/Parsed_Papers')\n",
    "\n",
    "# Define desired number of sentences for a summary\n",
    "NR_OF_SENTENCES = 5\n",
    "\n",
    "ground_truth_summaries = np.empty(1, dtype='object')\n",
    "generated_summaries = np.empty(1, dtype='object')\n",
    "\n",
    "for i, paper_file_name in tqdm(enumerate(paper_file_names[0:1])):\n",
    "    print(paper_file_name)\n",
    "    \n",
    "    # Read paper file\n",
    "    filepath = f'{PAPER_PATH}/{paper_file_name}'\n",
    "    with open(filepath, \"r\") as paper_file:\n",
    "        paper_content = paper_file.read()\n",
    "    \n",
    "    # Tokenize paper into sentences (and sentences into separate words) and get paper abstract\n",
    "    paper_sentences, tokenized_paper, paper_abstract = paper_tokenize(paper_content, sentences_as_lists=True, preserve_order=True)\n",
    "    ground_truth_summaries[i] = paper_abstract\n",
    "    \n",
    "    # Summarize paper\n",
    "    generated_summary = summarize_paper(tokenized_paper, paper_sentences, NR_OF_SENTENCES)\n",
    "    generated_summaries[i] = generated_paper\n",
    "    \n",
    "# Compute ROUGE scores\n",
    "compute_metrics(ground_truth_summaries, generated_summaries)\n",
    "print(generated_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e1413fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello my name is djesse. test sentence',\n",
       " 'hello my name is djesse. test sentence']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = [\"hello my name is djesse. test sentence\", \"hello my name is djesse. test sentence\" ]\n",
    "reference = [\"test sentence. hello my name is djesse\", \"test sentence. hello my name is djesse\"]\n",
    "\n",
    "hypothesis[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95187923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S0003687014000994.txt', 'S0377221714005463.txt', 'S0377221715003756.txt']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_file_names[0:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
