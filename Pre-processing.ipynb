{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a64a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1562562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERMITTED_TITLES_SOURCE = \"scientific-paper-summarisation/Data/Utility_Data/permitted_titles.txt\"\n",
    "non_content_keys = ['MAIN-TITLE', 'HIGHLIGHTS', 'KEYPHRASES', 'ABSTRACT', 'ACKNOWLEDGEMENTS', 'REFERENCES']\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7828d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, filter_sentence=True):\n",
    "    \"\"\"\n",
    "    Preprocesses a sentence, turning it all to lowercase and tokenizing it into words.\n",
    "    :param sentence: the sentence to pre-process.\n",
    "    :return: the sentence, as a list of words, all in lowercase\n",
    "    \"\"\"\n",
    "    \n",
    "    if filter_sentence:\n",
    "        sentence = sentence.lower()\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "\n",
    "        # Remove stopwords from sentence\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words and w.isalnum()]\n",
    "        return filtered_sentence\n",
    "    \n",
    "    # Remove all line endings, multiple whitespace etc. from sentence\n",
    "    cleaned_sentence = ' '.join(sentence.split())    \n",
    "    return cleaned_sentence\n",
    "\n",
    "def paper_tokenize(text, sentences_as_lists=False, preserve_order=False):\n",
    "    \"\"\"\n",
    "    Takes a paper with the sections delineated by '@&#' and splits them into a dictionary where the key is the section\n",
    "    and the value is the text under that section. This could probably be a bit more efficient but it works well enough.\n",
    "    :param text: the text of the paper to split\n",
    "    :param sentences_as_lists: if true, returns the text of each section as a list of sentences rather than a single\n",
    "                               string.\n",
    "    :param preserve_order: if true, tracks the order in which the paper sections occured.\n",
    "    :returns: a dictionary of the form (section: section_text)\n",
    "    \"\"\"\n",
    "    with open(PERMITTED_TITLES_SOURCE, \"r\") as pt:\n",
    "        permitted_titles = pt.read().split(\"\\n\")\n",
    "\n",
    "    # Split the text into sections\n",
    "    if preserve_order:\n",
    "        split_text_1 = re.split(\"@&#\", text)\n",
    "        split_text = zip(split_text_1, range(len(split_text_1)))\n",
    "    else:\n",
    "        split_text = re.split(\"@&#\", text)\n",
    "\n",
    "    # The key value. This value is changed if a permitted section title is encountered in the list.\n",
    "    state = \"\"\n",
    "\n",
    "    # After the for loop, this dictionary will have keys relating to each permitted section, and values corresponding\n",
    "    # to the text of that section\n",
    "    sentences_with_states = defaultdict(str)\n",
    "    sentences = defaultdict(str)\n",
    "\n",
    "    section_counts = defaultdict(int)\n",
    "\n",
    "    if preserve_order:\n",
    "        for text, pos in split_text:\n",
    "\n",
    "            # Hack for proper sentence tokenization because NLTK tokeniser doesn't work properly for tokenising papers\n",
    "            text = text.replace(\"etal.\", \"etal\")\n",
    "            text = text.replace(\"et al.\", \"etal\")\n",
    "            text = text.replace(\"Fig.\", \"Fig\")\n",
    "            text = text.replace(\"fig.\", \"fig\")\n",
    "            text = text.replace(\"Eq.\", \"Eq\")\n",
    "            text = text.replace(\"eq.\", \"eq\")\n",
    "            text = text.replace(\"pp.\", \"pp\")\n",
    "            text = text.replace(\"i.e.\", \"ie\")\n",
    "            text = text.replace(\"e.g.\", \"eg\")\n",
    "            text = text.replace(\"ref.\", \"ref\")\n",
    "            text = text.replace(\"Ref.\", \"Ref\")\n",
    "            text = text.replace(\"etc.\", \"etc\")\n",
    "            text = text.replace(\"Figs.\", \"Figs\")\n",
    "            text = text.replace(\"figs.\", \"figs\")\n",
    "            text = text.replace(\"No.\", \"No\")\n",
    "            text = text.replace(\"eqs.\", \"eqs\")\n",
    "\n",
    "            # Checks if text is a section title\n",
    "            if text.lower() in permitted_titles:\n",
    "                state = text\n",
    "                section_counts[state] += 1\n",
    "            else:\n",
    "                if sentences_as_lists:\n",
    "                    if section_counts[state] > 1:\n",
    "                        state = state + \"_\" + str(section_counts[state])\n",
    "                    sentences_with_states[state] = ([preprocess_sentence(x) for x in sent_tokenize(text)], pos)\n",
    "                    sentences[state] = [preprocess_sentence(x, filter_sentence=False) for x in sent_tokenize(text)]\n",
    "                else:\n",
    "                    if section_counts[state] > 1:\n",
    "                        state = state + \"_\" + str(section_counts[state])\n",
    "                    sentences_with_states[state] = (text, pos)\n",
    "\n",
    "    return sentences, sentences_with_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd58e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_as_words(tokenized_paper):\n",
    "    all_words = []\n",
    "    for key in tokenized_paper.keys():\n",
    "        \n",
    "        # For every paper section that contains content information,\n",
    "        # retrieve words\n",
    "        if key not in non_content_keys:\n",
    "            section_content = tokenized_paper[key]\n",
    "            [all_words.extend(s) for s in section_content[0]]\n",
    "            \n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86920414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_paper(tokenized_paper, paper_sentences, nr_sentences=5):\n",
    "    sentence_weights = []\n",
    "    \n",
    "    # Get word representation of paper\n",
    "    paper_words = get_paper_as_words(tokenized_paper)\n",
    "    \n",
    "    for section in tokenized_paper.keys():\n",
    "        \n",
    "        if section not in non_content_keys:\n",
    "            section_content = tokenized_paper[section]\n",
    "            section_sentences = paper_sentences[section]\n",
    "            \n",
    "            for tok_sentence, orig_sentence in zip(section_content[0], section_sentences):\n",
    "                # TODO compute sentence weight\n",
    "                rand_weight = random.randint(1, 100)\n",
    "                sentence_weights.append((orig_sentence, rand_weight))\n",
    "                pass\n",
    "            \n",
    "    # Create a dataframe of all sentences and sort descending by weight\n",
    "    sentence_weights = pd.DataFrame(sentence_weights, columns=['sentence', 'weight'])\n",
    "    sentence_weights.sort_values(by=['weight'], ascending=False, inplace=True)\n",
    "    \n",
    "    return sentence_weights.head(nr_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b0290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_PATH = 'scientific-paper-summarisation/Data/Parsed_Papers'\n",
    "paper_file_names = os.listdir('scientific-paper-summarisation/Data/Parsed_Papers')\n",
    "\n",
    "# Define desired number of sentences for a summary\n",
    "NR_OF_SENTENCES = 3\n",
    "\n",
    "for paper_file_name in tqdm(paper_file_names):\n",
    "    \n",
    "    # Read paper file\n",
    "    filepath = f'{PAPER_PATH}/{paper_file_name}'\n",
    "    with open(filepath, \"r\") as paper_file:\n",
    "        paper_content = paper_file.read()\n",
    "    \n",
    "    # Tokenize paper into sentences (and sentences into separate words)\n",
    "    paper_sentences, tokenized_paper = paper_tokenize(paper_content, sentences_as_lists=True, preserve_order=True)\n",
    "    \n",
    "    # Summarize paper\n",
    "    summary = summarize_paper(tokenized_paper, paper_sentences, NR_OF_SENTENCES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
