{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a64a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61dfa69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1562562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERMITTED_TITLES_SOURCE = \"scientific-paper-summarisation/Data/Utility_Data/permitted_titles.txt\"\n",
    "non_content_keys = ['MAIN-TITLE', 'HIGHLIGHTS', 'KEYPHRASES', 'ABSTRACT', 'ACKNOWLEDGEMENTS', 'REFERENCES']\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7828d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, filter_sentence=True):\n",
    "    \"\"\"\n",
    "    Preprocesses a sentence, turning it all to lowercase and tokenizing it into words.\n",
    "    :param sentence: the sentence to pre-process.\n",
    "    :return: the sentence, as a list of words, all in lowercase\n",
    "    \"\"\"\n",
    "    \n",
    "    if filter_sentence:\n",
    "        sentence = sentence.lower()\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "\n",
    "        # Remove stopwords from sentence\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words and w.isalnum()]\n",
    "        return filtered_sentence\n",
    "    \n",
    "    # Remove all line endings, multiple whitespace etc. from sentence\n",
    "    cleaned_sentence = ' '.join(sentence.split())    \n",
    "    return cleaned_sentence\n",
    "\n",
    "def paper_tokenize(text, sentences_as_lists=False, preserve_order=False):\n",
    "    \"\"\"\n",
    "    Takes a paper with the sections delineated by '@&#' and splits them into a dictionary where the key is the section\n",
    "    and the value is the text under that section. This could probably be a bit more efficient but it works well enough.\n",
    "    :param text: the text of the paper to split\n",
    "    :param sentences_as_lists: if true, returns the text of each section as a list of sentences rather than a single\n",
    "                               string.\n",
    "    :param preserve_order: if true, tracks the order in which the paper sections occured.\n",
    "    :returns: a dictionary of the form (section: section_text)\n",
    "    \"\"\"\n",
    "    with open(PERMITTED_TITLES_SOURCE, \"r\") as pt:\n",
    "        permitted_titles = pt.read().split(\"\\n\")\n",
    "\n",
    "    # Split the text into sections\n",
    "    if preserve_order:\n",
    "        split_text_1 = re.split(\"@&#\", text)\n",
    "        split_text = zip(split_text_1, range(len(split_text_1)))\n",
    "    else:\n",
    "        split_text = re.split(\"@&#\", text)\n",
    "\n",
    "    # The key value. This value is changed if a permitted section title is encountered in the list.\n",
    "    state = \"\"\n",
    "\n",
    "    # After the for loop, this dictionary will have keys relating to each permitted section, and values corresponding\n",
    "    # to the text of that section\n",
    "    sentences_with_states = defaultdict(str)\n",
    "    sentences = defaultdict(str)\n",
    "\n",
    "    section_counts = defaultdict(int)\n",
    "    \n",
    "    paper_abstract = \"\"\n",
    "\n",
    "    if preserve_order:\n",
    "        for text, pos in split_text:\n",
    "\n",
    "            # Hack for proper sentence tokenization because NLTK tokeniser doesn't work properly for tokenising papers\n",
    "            text = text.replace(\"etal.\", \"etal\")\n",
    "            text = text.replace(\"et al.\", \"etal\")\n",
    "            text = text.replace(\"Fig.\", \"Fig\")\n",
    "            text = text.replace(\"fig.\", \"fig\")\n",
    "            text = text.replace(\"Eq.\", \"Eq\")\n",
    "            text = text.replace(\"eq.\", \"eq\")\n",
    "            text = text.replace(\"pp.\", \"pp\")\n",
    "            text = text.replace(\"i.e.\", \"ie\")\n",
    "            text = text.replace(\"e.g.\", \"eg\")\n",
    "            text = text.replace(\"ref.\", \"ref\")\n",
    "            text = text.replace(\"Ref.\", \"Ref\")\n",
    "            text = text.replace(\"etc.\", \"etc\")\n",
    "            text = text.replace(\"Figs.\", \"Figs\")\n",
    "            text = text.replace(\"figs.\", \"figs\")\n",
    "            text = text.replace(\"No.\", \"No\")\n",
    "            text = text.replace(\"eqs.\", \"eqs\")\n",
    "\n",
    "            # Checks if text is a section title\n",
    "            if text.lower() in permitted_titles:\n",
    "                state = text\n",
    "                section_counts[state] += 1\n",
    "            else:\n",
    "                if sentences_as_lists:\n",
    "                    if section_counts[state] > 1:\n",
    "                        state = state + \"_\" + str(section_counts[state])\n",
    "                    sentences_with_states[state] = ([preprocess_sentence(x) for x in sent_tokenize(text)], pos)\n",
    "                    sentences[state] = [preprocess_sentence(x, filter_sentence=False) for x in sent_tokenize(text)]\n",
    "                else:\n",
    "                    if section_counts[state] > 1:\n",
    "                        state = state + \"_\" + str(section_counts[state])\n",
    "                    sentences_with_states[state] = (text, pos)\n",
    "            if state == \"ABSTRACT\":\n",
    "                paper_abstract = text.strip()\n",
    "\n",
    "    return sentences, sentences_with_states, paper_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd58e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_as_words(tokenized_paper):\n",
    "    all_words = []\n",
    "    for key in tokenized_paper.keys():\n",
    "        \n",
    "        # For every paper section that contains content information,\n",
    "        # retrieve words\n",
    "        if key not in non_content_keys:\n",
    "            section_content = tokenized_paper[key]\n",
    "            [all_words.extend(s) for s in section_content[0]]\n",
    "            \n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d74c1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_weight(sentence: List, text: List) -> int:\n",
    "    \n",
    "    sentence_score = 0\n",
    "    \n",
    "    # Iterate over all words in the sentence\n",
    "    for word in sentence:\n",
    "        \n",
    "        # Get word synsets\n",
    "        word_synsets = wn.synsets(word)\n",
    "        \n",
    "        # If word has synsets (i.e., is known by wordnet), continue\n",
    "        best_synset_score = 0\n",
    "        for synset in word_synsets:\n",
    "            \n",
    "            # Get and tokenize gloss and remove stopwords and punctuation\n",
    "            filtered_gloss = preprocess_sentence(synset.definition())\n",
    "            \n",
    "            # Compute score\n",
    "            score = sum(1 for def_word in filtered_gloss if def_word in text)\n",
    "            if score > best_synset_score:\n",
    "                best_synset_score = score\n",
    "                \n",
    "        # Update sentence score\n",
    "        sentence_score += best_synset_score\n",
    "\n",
    "    return sentence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86920414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_paper(tokenized_paper, paper_sentences, nr_sentences=5):\n",
    "    sentence_weights = []\n",
    "    \n",
    "    # Get word representation of paper\n",
    "    paper_words = get_paper_as_words(tokenized_paper)\n",
    "    \n",
    "    print(tokenized_paper.keys())\n",
    "    \n",
    "    for section in tokenized_paper.keys():\n",
    "        \n",
    "        if section not in non_content_keys:\n",
    "            section_content = tokenized_paper[section]\n",
    "            section_sentences = paper_sentences[section]\n",
    "            \n",
    "            for tok_sentence, orig_sentence in zip(section_content[0], section_sentences):\n",
    "                \n",
    "                # Compute sentence weight and store with sentence\n",
    "                sentence_weight = compute_sentence_weight(tok_sentence, paper_words)\n",
    "                sentence_weights.append((orig_sentence, sentence_weight))\n",
    "            \n",
    "    # Create a dataframe of all sentences and sort descending by weight\n",
    "    sentence_weights = pd.DataFrame(sentence_weights, columns=['sentence', 'weight'])\n",
    "    sentence_weights.sort_values(by=['weight'], ascending=False, inplace=True)\n",
    "    \n",
    "    summary = sentence_weights.head(nr_sentences)['sentence'].values\n",
    "    \n",
    "    string_summary = ' '.join(summary)\n",
    "    \n",
    "    \n",
    "    return string_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b2f36d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "def compute_metrics(paper_abstract: np.array, generated_summary: np.array):\n",
    "    rouge_scores = rouge.get_scores(generated_summary, paper_abstract, avg=True)\n",
    "    print(rouge_scores)\n",
    "#     return rouge_scores['rouge-1'].values(), rouge_scores['rouge-2'].values(), rouge_scores['rouge-l'].values(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26b0290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0003687014000994.txt\n",
      "dict_keys(['', 'MAIN-TITLE', 'HIGHLIGHTS', 'KEYPHRASES', 'ABSTRACT', 'INTRODUCTION', 'BACKGROUND', 'METHOD', 'RESULTS', 'DISCUSSION', 'CONCLUSION', 'ACKNOWLEDGEMENTS', 'REFERENCES'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:06,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0377221714005463.txt\n",
      "dict_keys(['', 'MAIN-TITLE', 'HIGHLIGHTS', 'KEYPHRASES', 'ABSTRACT'])\n",
      "S0377221715003756.txt\n",
      "dict_keys(['', 'MAIN-TITLE', 'HIGHLIGHTS', 'KEYPHRASES', 'ABSTRACT'])\n",
      "[\"We observed the following issues: • Patient wristband not present as it has fallen off, eg in the shower Patient wristband not present as the patient has removed it Patient wristband kept outside the room as they refused to wear it Patient barcode doesn't scan as it has been printed badly or is incomplete Patient barcode doesn't scan as the wristband has been put on awkwardly Patient slept on the arm with the wristband so they were disturbed and needed to move so staff could access it Patients sometimes manipulate their arm awkwardly so the barcode and scanner can be aligned In terms of the patient experience the last two were most noticeably, and the third issue suggests that at least some patients do not like wearing the wristband. An example of an innovation is thinking not only about individual glucose readings but instead the blood glucose reading round, eg the informal practice of scribbling bed numbers to attend to on bits of card and tissue paper could be formally listed on the device and ticked off as the blood glucose monitoring round is completed, see Issue 10 (Blood glucose rounds). (2012) differentiate their model by focussing it on “the role of the device with the aim of supporting future device designers in understanding the potential influences on the impact of medical device design.” For studies that take a socio-technical view of the world there will always be a link between the device and the system of work that surrounds it, and a view of the system of the work in which the device is embedded. This time period might expire if the member of staff is distracted by another task (eg an urgent request by a patient) or if part of the task takes too long (eg we observed instances where the healthcare assistant needed to notify the nurse looking after a particular patient that the reading was too high or too low but they were difficult to locate). (2012) highlight the significance of inpatient diabetes management including that poor glycemic control is associated with longer time in hospital and associated costs (Smith et al., 2009); the need for quality improvement in inpatient diabetes care (Daultrey etal, 2011); and that good diabetes management is a priority for healthcare organisations (Department of Health, 2001b).\"\n",
      " '' '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PAPER_PATH = 'scientific-paper-summarisation/Data/Parsed_Papers'\n",
    "paper_file_names = os.listdir('scientific-paper-summarisation/Data/Parsed_Papers')\n",
    "\n",
    "# Define desired number of sentences for a summary\n",
    "NR_OF_SENTENCES = 5\n",
    "\n",
    "ground_truth_summaries = np.empty(len(paper_file_names[1:4]), dtype='object')\n",
    "generated_summaries = np.empty(len(paper_file_names[1:4]), dtype='object')\n",
    "\n",
    "for i, paper_file_name in tqdm(enumerate(paper_file_names[0:3])):\n",
    "    print(paper_file_name)\n",
    "    \n",
    "    # Read paper file\n",
    "    filepath = f'{PAPER_PATH}/{paper_file_name}'\n",
    "    with open(filepath, \"r\") as paper_file:\n",
    "        paper_content = paper_file.read()\n",
    "    \n",
    "    # Tokenize paper into sentences (and sentences into separate words) and get paper abstract\n",
    "    paper_sentences, tokenized_paper, paper_abstract = paper_tokenize(paper_content, sentences_as_lists=True, preserve_order=True)\n",
    "    ground_truth_summaries[i] = paper_abstract\n",
    "    \n",
    "    # Summarize paper\n",
    "    generated_paper = summarize_paper(tokenized_paper, paper_sentences, NR_OF_SENTENCES)\n",
    "    generated_summaries[i] = generated_paper\n",
    "    \n",
    "# Compute ROUGE scores\n",
    "# compute_metrics(ground_truth_summaries, generated_summaries)\n",
    "print(generated_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb4c72e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello my name is djesse. test sentence',\n",
       " 'hello my name is djesse. test sentence']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = [\"hello my name is djesse. test sentence\", \"hello my name is djesse. test sentence\" ]\n",
    "reference = [\"test sentence. hello my name is djesse\", \"test sentence. hello my name is djesse\"]\n",
    "\n",
    "hypothesis[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "10a2942d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S0003687014000994.txt', 'S0377221714005463.txt', 'S0377221715003756.txt']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_file_names[0:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
